# SYLLABUS

**Unsupervised learning** - Data clustering and data transformations, engineering the data, overview of basic clustering methods, k-means clustering, fuzzy k-means clustering, expectation-maximization (EM) algorithm and gaussian mixtures clustering, some useful data transformations, entropy–based method for attribute discretization, principal components analysis (PCA) for attribute reduction, rough sets-based methods for attribute reduction. k-nearest neighbor (k-nn) classifier, discriminant functions and regression functions, linear regression with least square error criterion, logistic regression for classification tasks, fisher's linear discriminant and thresholding for classification, minimum description length principle.

## K means and fuzzy k means algo [CLICK!!][obsidian://open?vault=Notes&file=sem_6%2FAppliedMachineLearning%2FK%20means%20and%20Fuzzy%20C%20means]

## Gaussian Mixture Model

[YOUTUBE LINK!!][https://www.youtube.com/watch?v=wT2yLNUfyoM]

### Gaussian Mixture Model (GMM): An Overview

A **Gaussian Mixture Model (GMM)** is a probabilistic model that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters. It is widely used for clustering, density estimation, and as a generative model for complex data distributions.

### Key Concepts

[Rndom Image][https://repository-images.githubusercontent.com/260096455/47f1b200-8b2e-11ea-8fa1-ab106189aeb0]

1. **Gaussian Distribution**: Also known as the normal distribution, it is characterized by its mean (μ) and covariance matrix (Σ). In one dimension, the probability density function (pdf) of a Gaussian distribution is given by:
   ![[Pasted image 20240622214402.png]]

In multiple dimensions, the pdf is given by:
![[Pasted image 20240622214424.png]]

where \( x \) is a \( d \)-dimensional vector, \( \mu \) is the mean vector, and \( \Sigma \) is the covariance matrix.

2. **Mixture Model**: A mixture model represents the presence of subpopulations within an overall population, without requiring that an observed data point belong to a specific subpopulation. In GMM, each subpopulation is modeled as a Gaussian distribution.

3. **Latent Variables**: In GMM, a latent variable indicates which Gaussian distribution (or cluster) a particular data point belongs to.

### GMM Components

A GMM is parameterized by:

1. **Number of Components (K)**: The number of Gaussian distributions in the mixture.
2. **Means (μ_k)**: The mean vector of each Gaussian component \( k \).
3. **Covariances (Σ_k)**: The covariance matrix of each Gaussian component \( k \).
4. **Mixing Coefficients (π_k)**: The weights of each Gaussian component, which sum to 1.

The pdf of a GMM is given by:
![[Pasted image 20240622214458.png]]

### Expectation-Maximization (EM) Algorithm for GMM

The EM algorithm is typically used to estimate the parameters of a GMM. It iterates between two main steps until convergence:

1. **Expectation Step (E-step)**: Calculate the expected value of the latent variables given the current parameter estimates.

2. **Maximization Step (M-step)**: Maximize the expected log-likelihood with respect to the parameters.

### Detailed Steps of EM for GMM

1. **Initialization**:

   - Initialize the means, covariances, and mixing coefficients (e.g., using K-means clustering or randomly).

2. **E-step**:

   - Calculate the responsibilities \( \gamma\_{ik} \), which represent the probability that data point \( x_i \) was generated by Gaussian component \( k \):
     ![[Pasted image 20240622214522.png]]

3. **M-step**:
   - Update the parameters using the responsibilities:
     ![[Pasted image 20240622214545.png]]
4. **Check for Convergence**:
   - Check if the log-likelihood or parameters have converged. If not, return to the E-step.

### Example using `scikit-learn`

Here's an example of how to fit a GMM using the `scikit-learn` library:

```python
import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Generate sample data
X, y_true = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=42)

# Fit a Gaussian mixture model with EM
gmm = GaussianMixture(n_components=4, random_state=42)
gmm.fit(X)
labels = gmm.predict(X)

# Plot the data and the fitted GMM
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
plt.show()
```

### Advantages and Disadvantages

#### Advantages

- **Flexibility**: Can model any continuous distribution.
- **Clustering**: Can identify and cluster subpopulations within the data.
- **Probabilistic Assignments**: Provides probabilities for the assignment of data points to clusters.

#### Disadvantages

- **Complexity**: Requires the estimation of a large number of parameters, especially for high-dimensional data.
- **Initialization Sensitivity**: The algorithm can converge to local optima depending on the initial parameter values.
- **Computationally Intensive**: Can be slow to converge for large datasets.

### Conclusion

The Gaussian Mixture Model is a powerful tool for modeling data distributions and clustering. It extends the capabilities of simple Gaussian models by incorporating multiple components, each representing a different subpopulation within the data. The EM algorithm is commonly used to estimate the parameters of GMMs, providing a robust framework for handling incomplete data and uncovering the underlying structure of complex datasets.

## Principal Component Analysis (PCA): An Overview

**Principal Component Analysis (PCA)** is a statistical technique used for dimensionality reduction, data visualization, and noise reduction.
Principal component analysis (PCA) is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends.
It transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.

### Key Concepts

1. **Dimensionality Reduction**: Reducing the number of random variables under consideration, making the data easier to explore and visualize.
2. **Variance**: A measure of how much the data points deviate from the mean.
3. **Principal Components**: New uncorrelated variables that are linear combinations of the original variables, ordered by the amount of variance they capture from the data.

### Steps of PCA

![[Pasted image 20240622220305.png]]

##### CLICK- [STEPS OF PCA LINK!!][https://builtin.com/data-science/step-step-explanation-principal-component-analysis]

1. **Standardize the Data**: Ensure each feature has a mean of 0 and a standard deviation of 1. This step is crucial if the features have different units or scales.

   \[
   X\_{std} = \frac{X - \mu}{\sigma}
   \]

2. **Calculate the Covariance Matrix**: Compute the covariance matrix to understand how the variables interact with each other.

   \[
   \Sigma = \frac{1}{n-1} X*{std}^T X*{std}
   \]

3. **Compute the Eigenvalues and Eigenvectors**: The eigenvalues indicate the amount of variance carried by each principal component, and the eigenvectors represent the direction of these components.

   \[
   \Sigma v = \lambda v
   \]

   Here, \( \lambda \) represents the eigenvalues, and \( v \) represents the eigenvectors.

4. **Sort Eigenvalues and Eigenvectors**: Order them by decreasing eigenvalues. The eigenvector with the highest eigenvalue is the first principal component.

5. **Select Principal Components**: Choose the top \( k \) eigenvectors corresponding to the largest \( k \) eigenvalues to form a new matrix \( W \).

6. **Transform the Data**: Project the original data onto the new \( k \)-dimensional space.

   \[
   Y = X\_{std} W
   \]

### Mathematical Formulation

Given a dataset \( X \) with \( n \) samples and \( p \) features, the steps are:

1. **Standardize the Data**:

   \[
   X\_{std} = \frac{X - \mu}{\sigma}
   \]

2. **Covariance Matrix**:

   \[
   \Sigma = \frac{1}{n-1} X*{std}^T X*{std}
   \]

3. **Eigenvalues and Eigenvectors**:

   Solve \( \Sigma v = \lambda v \) for eigenvalues \( \lambda \) and eigenvectors \( v \).

4. **Sort and Select**:

   Sort eigenvalues in descending order and select the top \( k \) eigenvectors.

5. **Transform**:

   \[
   Y = X\_{std} W
   \]

### Advantages and Disadvantages

#### Advantages

- **Simplicity**: Easy to implement and computationally efficient.
- **Visualization**: Helps in visualizing high-dimensional data.
- **Noise Reduction**: Can reduce noise and improve the performance of machine learning algorithms.

#### Disadvantages

- **Linearity**: Assumes linear relationships between variables, which may not always hold.
- **Interpretability**: The new features (principal components) are linear combinations of the original features, which may not have a straightforward interpretation.
- **Information Loss**: Reducing dimensionality can lead to loss of information if not done carefully.

## Linear Discriminant Analysis

PCA is used for unsupervised classification problems while LCA is used for supervised classification problems.

### Linear Discriminant Analysis (LDA): An Overview

**Linear Discriminant Analysis (LDA)** is a classification and dimensionality reduction technique that projects the data onto a lower-dimensional space while maximizing the separability between different classes. It is particularly useful when the objective is to maximize class separability, making it a popular choice for supervised learning tasks.

### Key Concepts

1. **Class Separability**: LDA aims to maximize the distance between the means of different classes while minimizing the spread within each class.
2. **Scatter Matrices**: LDA uses within-class and between-class scatter matrices to measure the spread of the data.

### Steps of LDA

1. **Data Preparation:** Let’s say we have 150 iris samples with four features each, and the samples are evenly distributed among the three species.
2. **Compute Class Statistics:** Calculate the mean and covariance matrix for each feature in each class. This gives us three mean vectors and three covariance matrices (one for each class).
3. **Compute Between-Class and Within-Class Scatter Matrices:** Calculate the between-class scatter matrix by computing the differences between the mean vectors of each class and the overall mean, and then summing these outer products. Calculate the within-class scatter matrix by summing the covariance matrices of each class, weighted by the number of samples in each class.
4. **Compute Eigenvectors and Eigenvalues:** Solve the generalized eigenvalue problem using the between-class scatter matrix and the within-class scatter matrix. This gives us a set of eigenvectors and their corresponding eigenvalues.
5. **Select Discriminant Directions:** Sort the eigenvectors by their eigenvalues in descending order. Let’s say we want to reduce the dimensionality to 2, so we select the top two eigenvectors.
6. **Transform Data:** Project the original iris data onto the two selected eigenvectors. This gives us a new two-dimensional representation of the data.
7. **Classification:** In the reduced-dimensional space, we can use a classifier (e.g., k-nearest neighbors) to classify the iris flowers into one of the three species based on their positions in the reduced space.

### Mathematical Formulation

Given a dataset XXX with nnn samples and ppp features, divided into ccc classes, the steps are:

1. **Mean Vectors**:

   Calculate the mean vector for each class μk\mu_kμk​ and the overall mean vector μ\muμ:

   μk=1nk∑x∈Ckx\mu*k = \frac{1}{n_k} \sum*{x \in C*k} xμk​=nk​1​x∈Ck​∑​x μ=1n∑i=1nxi\mu = \frac{1}{n} \sum*{i=1}^{n} x_iμ=n1​i=1∑n​xi​

2. **Scatter Matrices**:

   - **Within-class scatter matrix SWS_WSW​**:
     SW=∑k=1c∑x∈Ck(x−μk)(x−μk)TS*W = \sum*{k=1}^{c} \sum\_{x \in C_k} (x - \mu_k)(x - \mu_k)^TSW​=k=1∑c​x∈Ck​∑​(x−μk​)(x−μk​)T
   - **Between-class scatter matrix SBS_BSB​**:
     SB=∑k=1cnk(μk−μ)(μk−μ)TS*B = \sum*{k=1}^{c} n_k (\mu_k - \mu)(\mu_k - \mu)^TSB​=k=1∑c​nk​(μk​−μ)(μk​−μ)T

3. **Eigenvalues and Eigenvectors**:

   Solve the generalized eigenvalue problem for SW−1SBS_W^{-1} S_BSW−1​SB​:

   SW−1SBv=λvS_W^{-1} S_B v = \lambda vSW−1​SB​v=λv

4. **Sort and Select**:

   Sort eigenvalues in descending order and select the top kkk eigenvectors to form a new matrix WWW.

5. **Transform**:

   Project the original data onto the new kkk-dimensional space:

   Y=XWY = X WY=XW

